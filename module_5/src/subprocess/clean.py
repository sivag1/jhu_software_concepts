"""Data Cleaning Module for GradCafe Applicant Data.

This module provides functionality to clean and standardize raw applicant data
scraped from The GradCafe website. It removes HTML remnants, normalizes missing
data, and parses statistical information (GPA, GRE scores, etc.) from raw text.

Author: Siva Govindarajan
Date: January 2026
"""

import json
import re
import os


class DataCleaner:
    """Cleans and standardizes GradCafe applicant data.

    This class handles loading raw applicant data, removing HTML remnants,
    normalizing missing values, extracting structured data from raw text,
    and saving cleaned results to JSON.

    Attributes:
        data (list): List of applicant records loaded from JSON file.
    """
    def __init__(self):
        """Initialize the DataCleaner with an empty data list."""
        self.data = []

    def load_data(self, filename="applicant_data.json"):
        """
        Loads raw data from the JSON file generated by scrape.py.
        """
        if not os.path.exists(filename):
            print(f"Error: {filename} not found. Run scrape.py first.")
            return

        with open(filename, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        print(f"Loaded {len(self.data)} entries from {filename}")

    def clean_data(self):
        """Iterates through the dataset to sanitize fields.

        Cleans each entry by calling specialized cleaning methods for
        different field types. Meets requirement:
        'SHOULD ensure data does not include any remnant HTML'.
        """
        cleaned_list = []

        for entry in self.data:
            cleaned_entry = {}
            for key, value in entry.items():
                if key == "program":
                    # Special handling for program field to remove degree keywords
                    cleaned_entry[key] = self._clean_program(value)
                elif key != "raw":
                    # Clean standard fields
                    cleaned_entry[key] = self._clean_value(value)
                else:
                    # Parse statistics from raw text field
                    extra_data = self.parse_stats(value)
                    for extra_key, extra_value in extra_data.items():
                        cleaned_entry[extra_key] = self._clean_value(extra_value)
            cleaned_list.append(cleaned_entry)

        self.data = cleaned_list
        print("Data sanitization complete.")

    def _clean_value(self, value):
        """Clean individual string values.

        Private method to sanitize field values by removing HTML tags,
        normalizing whitespace, and standardizing missing data.

        Args:
            value: The value to clean (can be None, string, or other type).

        Returns:
            str: Cleaned value, or original value if not a string.

        Cleaning steps:
            1. Remove HTML tags using regex
            2. Replace HTML entities and control characters
            3. Collapse multiple spaces into one
            4. Strip leading/trailing whitespace
            5. Standardize missing data formats to empty string ""
        """
        if value is None:
            return ""

        if not isinstance(value, str):
            return value

        # Step 1: Remove remnant HTML tags (e.g., <span>, <br>, <a>)
        value = re.sub(r'<[^>]+>', '', value)

        # Step 2: Replace HTML entities, newlines, tabs with spaces
        value = value.replace('&nbsp;', ' ').replace('\n', ' ').replace('\r', '').replace('\t', ' ')

        # Step 3: Collapse multiple consecutive spaces into one
        value = re.sub(r'\s+', ' ', value)

        # Step 4: Strip leading and trailing whitespace
        value = value.strip()

        # Step 5: Standardize unavailable/missing data to empty string
        # Meets requirement: 'SHOULD ensure unavailable data is maintained in a consistent format'
        if value.lower() in ["n/a", "na", "none", "unknown", ""]:
            return ""

        return value

    def _clean_program(self, value):
        """Clean the program field by removing redundant degree information.

        Since degree information is already captured in a separate 'degree'
        field, this method removes degree keywords to avoid duplication and
        improve data consistency.

        Args:
            value (str): Raw program name string.

        Returns:
            str: Cleaned program name with degree keywords removed.
        """
        value = self._clean_value(value)

        # Remove common degree keywords (PhD, Masters, M.S., M.A., etc.)
        value = re.sub(
            r"\s*(PhD|Master'?s?|M\.S\.|M\.A\.|M\.B\.A\.|M\.D\.|M\.E\.)\s*",
            ' ', value, flags=re.I
        )

        # Clean up any resulting multiple spaces
        value = re.sub(r'\s+', ' ', value).strip()

        return value

    def save_data(self, filename="cleaned_applicant_data.json"):
        """Save the sanitized data to a JSON file.

        Args:
            filename (str): Output file path. Defaults to 'cleaned_applicant_data.json'.
        """
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, indent=4)
        print(f"Sanitized data saved to {filename}")

    def parse_stats(self, stats_text):
        """Parse applicant statistics from raw text.

        Extracts structured data (GPA, GRE scores, student type, term) from
        raw text using regex patterns. This is used during cleaning to populate
        additional fields.

        Args:
            stats_text (str): Raw statistics text from scraped data.

        Returns:
            dict: Dictionary with extracted fields like 'gpa', 'greScore',
                  'greV', 'greAW', 'US/International', 'term'.
        """
        data = {}

        # Extract GPA (format: "GPA 3.91" or similar)
        gpa_match = re.search(r'GPA\s*([\d\.]+)', stats_text, re.I)
        if gpa_match:
            data['gpa'] = gpa_match.group(1)

        # Extract GRE total score (format: "GRE 157" or similar)
        gre_match = re.search(r'GRE\s*(\d+)', stats_text, re.I)
        if gre_match:
            data['greScore'] = gre_match.group(1)

        # Extract GRE Verbal score (format: "GRE V 163" or similar)
        gre_v_match = re.search(r'GRE V\s*(\d+)', stats_text, re.I)
        if gre_v_match:
            data['greV'] = gre_v_match.group(1)

        # Extract GRE Analytical Writing score (format: "GRE AW 4.5" or similar)
        gre_aw_match = re.search(r'GRE AW\s*([\d\.]+)', stats_text, re.I)
        if gre_aw_match:
            data['greAW'] = gre_aw_match.group(1)

        # Extract student type (International or American)
        if 'International' in stats_text:
            data['US/International'] = 'International'
        elif 'American' in stats_text:
            data['US/International'] = 'American'

        # Extract semester and year (format: "Fall 2026" or similar)
        sem_year = re.search(r'(Fall|Spring|Summer|Winter)\s*(\d{4})', stats_text, re.I)
        if sem_year:
            # Combined term field
            data['term'] = sem_year.group(1) + " " + sem_year.group(2)

        return data


if __name__ == "__main__":
    # Initialize cleaner and run the data cleaning pipeline
    cleaner = DataCleaner()

    # Step 1: Load raw data from the scraper output
    cleaner.load_data("applicant_data.json")

    # Step 2: Clean and standardize all fields
    cleaner.clean_data()

    # Step 3: Save cleaned data to output file
    cleaner.save_data("cleaned_applicant_data.json")
